{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <h1>\n",
    "        VSL RL : SUMO Simulation\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asmae/opt/anaconda3/lib/python3.9/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('C:\\PHD\\Workspace\\gym_sumo_vsl_maroc\\gym_sumo\\envs')\n",
    "from utils import plot_policy, plot_action_values, test_agent\n",
    "import SUMOInitializeEnv\n",
    "import gym\n",
    "import sumo_env as env\n",
    "import ql_agent as QLAgent\n",
    "import epsilon_greedy as EpsilonGreedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gym.envs.registration import register\n",
    "from SUMOInitializeEnv import SUMOEnv_Initializer\n",
    "register(\n",
    "    id='SumoGUI-v0',\n",
    "    entry_point='SUMOInitializeEnv:SUMOEnv_Initializer'\n",
    ")\n",
    "env = gym.make('SumoGUI-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_uniform_grid(low,high, bins=(500,500)):\n",
    "        grid= []\n",
    "        for i, lower_upper in enumerate(zip(low,high)):\n",
    "            grid_column= np.linspace(lower_upper[0], lower_upper[1], bins[i]+1)\n",
    "            grid.append(grid_column)\n",
    "        return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0\n",
      " Retrying in 1 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fontconfig warning: ignoring UTF-8: not a valid region tag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximizing action\n",
      "Current action = 19, current state ()\n",
      "act action : 19\n",
      "reward: 2.2559700065722175\n",
      "current reward: 2.2559700065722175; current state: [2.0, 0.0028441410693970416]\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/1b/4z7ybqpn7zvd78k3zrqnskv40000gn/T/ipykernel_30179/1938342525.py\", line 24, in <module>\n",
      "    ql_agents.learn(next_state=s, reward=r)\n",
      "  File \"/Users/asmae/Desktop/PHD/rl_vsl_sumo/rl_vsl_sumo/gym_sumo/envs/ql_agent.py\", line 46, in learn\n",
      "    temp= reward + self.gamma * max(self.q_table[s1])\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "action_space= np.array([19, 22, 25, 28, 31, 33])\n",
    "try:\n",
    "    for i in range(1):#episodes\n",
    "        print(f\"episode: {i}\")\n",
    "        initial_states = env.reset()\n",
    "        #print(env.observation_space)\n",
    "        action_space_size = len(env.action_space)\n",
    "        #print(f\"action_space_size{action_space_size}\")\n",
    "        for t in range(1000):#time steps\n",
    "            state_grid= create_uniform_grid(env.observation_space.low, env.observation_space.high, bins= (400,400))\n",
    "            ql_agents = QLAgent.QLAgent(starting_state=initial_states,\n",
    "                                    state_space=env.observation_space,\n",
    "                                    state_grid = state_grid,\n",
    "                                    action_space=env.action_space,\n",
    "                                    alpha=0.2,\n",
    "                                    gamma=0.99,\n",
    "                                    exploration_strategy=EpsilonGreedy.EpsilonGreedy())\n",
    "\n",
    "            actions =ql_agents.act()\n",
    "            print(f\"act action : {actions}\")\n",
    "            s, r, done, _ = env.step(action=actions)\n",
    "            print(f\"current reward: {r}; current state: {s}\")\n",
    "            ql_agents.learn(next_state=s, reward=r)\n",
    "            if done:\n",
    "                break\n",
    "        env.close()\n",
    "except ValueError:\n",
    "    print(traceback.format_exc())\n",
    "    env.close()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88117a4c4f7db09762e85aecb2e581e7c8f40331f8439cb18f9752f946649d6e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
