{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <h1>\n",
    "        VSL RL : SUMO Simulation\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asmae/opt/anaconda3/lib/python3.9/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('C:\\PHD\\Workspace\\gym_sumo_vsl_maroc\\gym_sumo\\envs')\n",
    "from utils import plot_policy, plot_action_values, test_agent\n",
    "import SUMOInitializeEnv\n",
    "import gym\n",
    "import sumo_env as env\n",
    "import ql_agent as QLAgent\n",
    "import epsilon_greedy as EpsilonGreedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gym.envs.registration import register\n",
    "from SUMOInitializeEnv import SUMOEnv_Initializer\n",
    "register(\n",
    "    id='SumoGUI-v0',\n",
    "    entry_point='SUMOInitializeEnv:SUMOEnv_Initializer'\n",
    ")\n",
    "env = gym.make('SumoGUI-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_uniform_grid(low,high, bins=(500,500)):\n",
    "        grid= []\n",
    "        for i, lower_upper in enumerate(zip(low,high)):\n",
    "            grid_column= np.linspace(lower_upper[0], lower_upper[1], bins[i]+1)\n",
    "            grid.append(grid_column)\n",
    "        return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0\n",
      " Retrying in 1 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fontconfig warning: ignoring UTF-8: not a valid region tag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (3, 3)\n",
      "act action : 19\n",
      "reward: 15.415118587086909\n",
      "current reward: 15.415118587086909; current state: [2.0, 0.0028441410693970416]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (11, 3)\n",
      "act action : 19\n",
      "reward: 16.694056802129488\n",
      "current reward: 16.694056802129488; current state: [2.0, 0.0028441410693970416]\n",
      "maximizing action\n",
      "[3.33881136 0.         0.         0.         0.         0.        ]\n",
      "Current action = 19, current state (11, 3)\n",
      "act action : 19\n",
      "reward: 17.187356904678317\n",
      "current reward: 17.187356904678317; current state: [2.0, 0.0028441410693970416]\n",
      "maximizing action\n",
      "[6.77628274 0.         0.         0.         0.         0.        ]\n",
      "Current action = 19, current state (11, 3)\n",
      "act action : 19\n",
      "reward: 14.175584437173244\n",
      "current reward: 14.175584437173244; current state: [3.0, 0.004266211604095562]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (15, 3)\n",
      "act action : 19\n",
      "reward: 14.684790846782036\n",
      "current reward: 14.684790846782036; current state: [3.0, 0.004266211604095562]\n",
      "maximizing action\n",
      "[2.93695817 0.         0.         0.         0.         0.        ]\n",
      "Current action = 19, current state (15, 3)\n",
      "act action : 19\n",
      "reward: 12.744555742636429\n",
      "current reward: 12.744555742636429; current state: [4.0, 0.005688282138794083]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (19, 3)\n",
      "act action : 19\n",
      "reward: 13.222667778244931\n",
      "current reward: 13.222667778244931; current state: [4.0, 0.005688282138794083]\n",
      "maximizing action\n",
      "[2.64453356 0.         0.         0.         0.         0.        ]\n",
      "Current action = 19, current state (19, 3)\n",
      "act action : 19\n",
      "reward: 13.987080191508857\n",
      "current reward: 13.987080191508857; current state: [4.0, 0.005688282138794083]\n",
      "maximizing action\n",
      "[5.44194959 0.         0.         0.         0.         0.        ]\n",
      "Current action = 19, current state (19, 3)\n",
      "act action : 19\n",
      "reward: 13.558385406056379\n",
      "current reward: 13.558385406056379; current state: [5.0, 0.007110352673492605]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (23, 3)\n",
      "act action : 19\n",
      "reward: 14.092611808165865\n",
      "current reward: 14.092611808165865; current state: [5.0, 0.007110352673492605]\n",
      "maximizing action\n",
      "[2.81852236 0.         0.         0.         0.         0.        ]\n",
      "Current action = 19, current state (23, 3)\n",
      "act action : 19\n",
      "reward: 13.852892725737616\n",
      "current reward: 13.852892725737616; current state: [6.0, 0.008532423208191125]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (27, 3)\n",
      "act action : 19\n",
      "reward: 14.68408868580802\n",
      "current reward: 14.68408868580802; current state: [6.0, 0.008532423208191125]\n",
      "maximizing action\n",
      "[2.93681774 0.         0.         0.         0.         0.        ]\n",
      "Current action = 19, current state (27, 3)\n",
      "act action : 19\n",
      "reward: 15.12659906867009\n",
      "current reward: 15.12659906867009; current state: [7.0, 0.009954493742889647]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (31, 3)\n",
      "act action : 19\n",
      "reward: 15.67128149076647\n",
      "current reward: 15.67128149076647; current state: [6.555555555555555, 4.095296576667182]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (29, 19)\n",
      "act action : 19\n",
      "reward: 16.58276651759034\n",
      "current reward: 16.58276651759034; current state: [7.0, 0.009954493742889647]\n",
      "maximizing action\n",
      "[3.1342563 0.        0.        0.        0.        0.       ]\n",
      "Current action = 19, current state (31, 3)\n",
      "act action : 19\n",
      "reward: 15.40515099447198\n",
      "current reward: 15.40515099447198; current state: [8.0, 0.011376564277588166]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (35, 3)\n",
      "act action : 19\n",
      "reward: 16.19554660107506\n",
      "current reward: 16.19554660107506; current state: [8.0, 0.011376564277588166]\n",
      "maximizing action\n",
      "[3.23910932 0.         0.         0.         0.         0.        ]\n",
      "Current action = 19, current state (35, 3)\n",
      "act action : 19\n",
      "reward: 17.4327125677114\n",
      "current reward: 17.4327125677114; current state: [9.0, 0.012798634812286692]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (39, 3)\n",
      "act action : 19\n",
      "reward: 18.124319672695183\n",
      "current reward: 18.124319672695183; current state: [8.703703703703704, 2.162928260215189]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (37, 11)\n",
      "act action : 19\n",
      "reward: 19.009990069663978\n",
      "current reward: 19.009990069663978; current state: [8.407407407407407, 3.884547805667697]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (35, 17)\n",
      "act action : 19\n",
      "reward: 19.53398789582182\n",
      "current reward: 19.53398789582182; current state: [9.666666666666666, 2.170983678466381]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (41, 11)\n",
      "act action : 19\n",
      "reward: 20.483057998642224\n",
      "current reward: 20.483057998642224; current state: [10.0, 0.01422070534698521]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (43, 3)\n",
      "act action : 19\n",
      "reward: 20.384992032972853\n",
      "current reward: 20.384992032972853; current state: [10.25925925925926, 1.0392699697165133]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (43, 7)\n",
      "act action : 19\n",
      "reward: 21.089245421357806\n",
      "current reward: 21.089245421357806; current state: [10.25925925925926, 0.01564277588168373]\n",
      "maximizing action\n",
      "[4.07699841 0.         0.         0.         0.         0.        ]\n",
      "Current action = 19, current state (43, 3)\n",
      "act action : 19\n",
      "reward: 20.298528859252844\n",
      "current reward: 20.298528859252844; current state: [11.185185185185185, 4.17484591123953]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (47, 19)\n",
      "act action : 19\n",
      "reward: 21.18713247533549\n",
      "current reward: 21.18713247533549; current state: [11.592592592592593, 0.01706484641638225]\n",
      "exploring action\n",
      "Current action = 25, current state (49, 3)\n",
      "act action : 25\n",
      "reward: 22.059096905183072\n",
      "current reward: 22.059096905183072; current state: [12.0, 0.01706484641638225]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (51, 3)\n",
      "act action : 19\n",
      "reward: 22.955041661669966\n",
      "current reward: 22.955041661669966; current state: [12.0, 0.01706484641638225]\n",
      "maximizing action\n",
      "[4.59100833 0.         0.         0.         0.         0.        ]\n",
      "Current action = 19, current state (51, 3)\n",
      "act action : 19\n",
      "reward: 22.681253044706285\n",
      "current reward: 22.681253044706285; current state: [13.0, 0.018486916951080776]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (55, 3)\n",
      "act action : 19\n",
      "reward: 22.94522575579482\n",
      "current reward: 22.94522575579482; current state: [13.037037037037036, 3.6125529979381663]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (55, 17)\n",
      "act action : 19\n",
      "reward: 23.495244245381006\n",
      "current reward: 23.495244245381006; current state: [12.555555555555555, 1.9263516002642638]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (53, 9)\n",
      "act action : 19\n",
      "reward: 26.86798559994497\n",
      "current reward: 26.86798559994497; current state: [13.037037037037036, 1.1817515310784255]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (55, 7)\n",
      "act action : 19\n",
      "reward: 23.688328254788697\n",
      "current reward: 23.688328254788697; current state: [13.444444444444445, 0.05321910237608685]\n",
      "maximizing action\n",
      "[4.58904515 0.         0.         0.         0.         0.        ]\n",
      "Current action = 19, current state (55, 3)\n",
      "act action : 19\n",
      "reward: 23.780809637020962\n",
      "current reward: 23.780809637020962; current state: [14.481481481481481, 0.021331058020477817]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (59, 3)\n",
      "act action : 19\n",
      "reward: 23.942180743734202\n",
      "current reward: 23.942180743734202; current state: [14.333333333333334, 3.7783111107685188]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (59, 17)\n",
      "act action : 19\n",
      "reward: 24.312056872154688\n",
      "current reward: 24.312056872154688; current state: [13.777777777777779, 4.240455545421278]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (57, 19)\n",
      "act action : 19\n",
      "reward: 23.129155619294693\n",
      "current reward: 23.129155619294693; current state: [15.814814814814815, 0.02417519908987486]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (65, 3)\n",
      "act action : 19\n",
      "reward: 23.49431568579332\n",
      "current reward: 23.49431568579332; current state: [15.814814814814815, 4.298013609492603]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (65, 19)\n",
      "act action : 19\n",
      "reward: 23.78866897966749\n",
      "current reward: 23.78866897966749; current state: [17.0, 0.02417519908987486]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (71, 3)\n",
      "act action : 19\n",
      "reward: 22.979707383409693\n",
      "current reward: 22.979707383409693; current state: [17.37037037037037, 0.4668278210391597]\n",
      "maximizing action\n",
      "[4.59594148 0.         0.         0.         0.         0.        ]\n",
      "Current action = 19, current state (71, 3)\n",
      "act action : 19\n",
      "reward: 23.474765782823763\n",
      "current reward: 23.474765782823763; current state: [16.74074074074074, 1.918113159250008]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (69, 9)\n",
      "act action : 19\n",
      "reward: 24.13688181456804\n",
      "current reward: 24.13688181456804; current state: [17.666666666666668, 3.037835059456096]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (73, 15)\n",
      "act action : 19\n",
      "reward: 24.597361457847956\n",
      "current reward: 24.597361457847956; current state: [18.333333333333332, 0.027019340159271897]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (75, 3)\n",
      "act action : 19\n",
      "reward: 25.093692797319544\n",
      "current reward: 25.093692797319544; current state: [17.0, 5.24734673625021]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (71, 23)\n",
      "act action : 19\n",
      "reward: 24.600885173327523\n",
      "current reward: 24.600885173327523; current state: [18.59259259259259, 1.9821894078338318]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (77, 9)\n",
      "act action : 19\n",
      "reward: 26.988318441988856\n",
      "current reward: 26.988318441988856; current state: [18.59259259259259, 2.6121480325140483]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (77, 13)\n",
      "act action : 19\n",
      "reward: 25.788132226847903\n",
      "current reward: 25.788132226847903; current state: [20.25925925925926, 1.9939993681537544]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (83, 9)\n",
      "act action : 19\n",
      "reward: 26.162792122661514\n",
      "current reward: 26.162792122661514; current state: [16.555555555555557, 8.250831673120576]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (69, 35)\n",
      "act action : 19\n",
      "reward: 27.66807899561972\n",
      "current reward: 27.66807899561972; current state: [19.666666666666668, 2.12051077253826]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (81, 11)\n",
      "act action : 19\n",
      "reward: 25.687603854960273\n",
      "current reward: 25.687603854960273; current state: [19.666666666666668, 0.6312134506583107]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (81, 5)\n",
      "act action : 19\n",
      "reward: 26.65323076221709\n",
      "current reward: 26.65323076221709; current state: [18.88888888888889, 3.12873181234615]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (77, 15)\n",
      "act action : 19\n",
      "reward: 26.454956000389902\n",
      "current reward: 26.454956000389902; current state: [20.444444444444443, 0.03128555176336746]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (83, 3)\n",
      "act action : 19\n",
      "reward: 27.226542911644795\n",
      "current reward: 27.226542911644795; current state: [21.37037037037037, 2.5872411940952955]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (87, 13)\n",
      "act action : 19\n",
      "reward: 26.347735457443317\n",
      "current reward: 26.347735457443317; current state: [21.444444444444443, 3.782443474500484]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (87, 17)\n",
      "act action : 19\n",
      "reward: 25.900544348882452\n",
      "current reward: 25.900544348882452; current state: [21.444444444444443, 2.386395444901592]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (87, 11)\n",
      "act action : 19\n",
      "reward: 26.917404837109398\n",
      "current reward: 26.917404837109398; current state: [21.444444444444443, 0.0341296928327645]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (87, 3)\n",
      "act action : 19\n",
      "reward: 26.15509346324048\n",
      "current reward: 26.15509346324048; current state: [24.11111111111111, 1.559111902807366]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (99, 9)\n",
      "act action : 19\n",
      "reward: 26.90792157941836\n",
      "current reward: 26.90792157941836; current state: [23.22222222222222, 2.3389479906614827]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (95, 11)\n",
      "act action : 19\n",
      "reward: 25.667669535866782\n",
      "current reward: 25.667669535866782; current state: [22.296296296296298, 6.267965287553573]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (91, 27)\n",
      "act action : 19\n",
      "reward: 25.97336521050467\n",
      "current reward: 25.97336521050467; current state: [23.22222222222222, 4.40080609070438]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (95, 19)\n",
      "act action : 19\n",
      "reward: 26.29460293716782\n",
      "current reward: 26.29460293716782; current state: [24.11111111111111, 6.694564818544562]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (99, 29)\n",
      "act action : 19\n",
      "reward: 27.555260297308607\n",
      "current reward: 27.555260297308607; current state: [25.074074074074073, 2.4205320767710568]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (103, 11)\n",
      "act action : 19\n",
      "reward: 26.6802239225686\n",
      "current reward: 26.6802239225686; current state: [26.037037037037038, 2.029407925246823]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (107, 11)\n",
      "act action : 19\n",
      "reward: 26.09340557988807\n",
      "current reward: 26.09340557988807; current state: [23.0, 4.068576873391687]\n",
      "maximizing action\n",
      "[5.25892059 0.         0.         0.         0.         0.        ]\n",
      "Current action = 19, current state (95, 19)\n",
      "act action : 19\n",
      "reward: 27.370789194881265\n",
      "current reward: 27.370789194881265; current state: [25.0, 0.03981797497155859]\n",
      "maximizing action\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Current action = 19, current state (103, 3)\n",
      "act action : 19\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "action_space= np.array([19, 22, 25, 28, 31, 33])\n",
    "try:\n",
    "    for i in range(1):#episodes\n",
    "        print(f\"episode: {i}\")\n",
    "        initial_states = env.reset()\n",
    "        #print(env.observation_space)\n",
    "        action_space_size = len(env.action_space)\n",
    "        state_grid= create_uniform_grid(env.observation_space.low, env.observation_space.high, bins= (400,400))\n",
    "        ql_agents = QLAgent.QLAgent(starting_state=initial_states,\n",
    "                                    state_space=env.observation_space,\n",
    "                                    state_grid = state_grid,\n",
    "                                    action_space=env.action_space,\n",
    "                                    alpha=0.2,\n",
    "                                    gamma=0.99,\n",
    "                                    exploration_strategy=EpsilonGreedy.EpsilonGreedy())\n",
    "        #print(f\"action_space_size{action_space_size}\")\n",
    "        for t in range(1000):#time steps\n",
    "            actions =ql_agents.act()\n",
    "            print(f\"act action : {actions}\")\n",
    "            s, r, done, _ = env.step(action=actions)\n",
    "            print(f\"current reward: {r}; current state: {s}\")\n",
    "            ql_agents.learn(next_state=s, reward=r)\n",
    "            if done:\n",
    "                break\n",
    "        env.close()\n",
    "except ValueError:\n",
    "    print(traceback.format_exc())\n",
    "    env.close()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88117a4c4f7db09762e85aecb2e581e7c8f40331f8439cb18f9752f946649d6e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
